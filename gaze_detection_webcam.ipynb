{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gaze_tracking opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1ImFPPJkXox",
        "outputId": "6c921e35-a32b-4e55-b52a-073ef4e4efe3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gaze_tracking\n",
            "  Downloading gaze_tracking-0.0.1-py3-none-any.whl.metadata (595 bytes)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Downloading gaze_tracking-0.0.1-py3-none-any.whl (1.3 kB)\n",
            "Installing collected packages: gaze_tracking\n",
            "Successfully installed gaze_tracking-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1wd25evkzI9",
        "outputId": "21664b43-4b95-43aa-a317-bd9abea2d75f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaze tracking by webcam (murtaza)"
      ],
      "metadata": {
        "id": "k9lA5ABys6ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "from imutils import face_utils\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, Javascript # Import display and Javascript\n",
        "\n",
        "# Load detector and predictor\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "left_eye_points = [36, 37, 38, 39, 40, 41]\n",
        "right_eye_points = [42, 43, 44, 45, 46, 47]\n",
        "\n",
        "def get_eye_region(landmarks, eye_points):\n",
        "    points = [landmarks.part(i) for i in eye_points]\n",
        "    return np.array([(p.x, p.y) for p in points], dtype=np.int32)\n",
        "\n",
        "# Function to take photo in Google Colab\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture Photo';\n",
        "      div.append(capture);\n",
        "\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      let preview = document.createElement('video');\n",
        "      preview.title = 'Click to stop preview';\n",
        "      preview.style.display = 'block';\n",
        "      preview.srcObject = stream;\n",
        "      document.body.append(div);\n",
        "      div.append(preview);\n",
        "      await preview.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = preview.videoWidth;\n",
        "      canvas.height = preview.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(preview, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = base64.b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "# Import necessary functions from google.colab for the take_photo utility\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "\n",
        "filename = take_photo() # Call the defined take_photo function\n",
        "image = cv2.imread(filename)\n",
        "\n",
        "# Check if the image was loaded successfully\n",
        "if image is None:\n",
        "    print(f\"Error: Unable to load image from {filename}\")\n",
        "else:\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "\n",
        "        for eye_points in [left_eye_points, right_eye_points]:\n",
        "            region = get_eye_region(landmarks, eye_points)\n",
        "            x, y, w, h = cv2.boundingRect(region)\n",
        "            eye = gray[y:y+h, x:x+w]\n",
        "\n",
        "            if eye.shape[0] == 0 or eye.shape[1] == 0:\n",
        "                continue\n",
        "\n",
        "            eye = cv2.resize(eye, (100, 50))\n",
        "\n",
        "            # Adjusted threshold value here\n",
        "            _, threshold_eye = cv2.threshold(eye, 60, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "            # Show thresholded eye image for debugging\n",
        "            cv2_imshow(threshold_eye)\n",
        "\n",
        "            moments = cv2.moments(threshold_eye)\n",
        "\n",
        "            if moments['m00'] != 0:\n",
        "                cx = int(moments['m10'] / moments['m00'])\n",
        "\n",
        "                print(\"Centroid x:\", cx)  # Debug print\n",
        "\n",
        "                # Adjusted left/right thresholds here\n",
        "                if cx < 45:\n",
        "                    direction = \"Looking Left\"\n",
        "                elif cx > 51:\n",
        "                    direction = \"Looking Right\"\n",
        "                else:\n",
        "                    direction = \"Looking Center\"\n",
        "\n",
        "                cv2.putText(image, direction, (face.left(), face.top() - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "\n",
        "    cv2_imshow(image)"
      ],
      "metadata": {
        "id": "Too1_svKEwOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}